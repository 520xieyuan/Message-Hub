# LLM 消息总结功能 - 方案需求与实施计划

## 1. 需求概述

### 1.1 功能目标

为跨平台消息搜索应用添加 **AI 总结功能**，用户可以将搜索结果交给本地 LLM 模型进行智能总结，快速获取关键信息。

### 1.2 使用场景

- 用户搜索某个关键词，得到多条来自不同平台（Slack、Gmail、Lark）的消息
- 点击"AI 总结"按钮，系统将消息内容发送给本地 LLM
- LLM 返回结构化的总结内容（要点提取、关键信息、行动项等）

### 1.3 核心价值

| 价值点 | 说明 |
|--------|------|
| 提升效率 | 快速理解大量消息，无需逐条阅读 |
| 数据隐私 | 本地运行，消息不上传云端 |
| 零成本 | 使用开源模型，无 API 费用 |

---

## 2. 技术方案

### 2.1 技术选型

| 组件 | 选择 | 理由 |
|------|------|------|
| LLM 运行时 | Ollama（远程服务器） | API 简单、独立部署不占用本机资源 |
| 模型 | qwen2.5:7b | 中文能力强、128K 上下文 |
| 调用方式 | Electron 主进程 | 安全、复用现有架构 |

### 2.2 架构设计

```
┌─────────────────────────────────────────────────────────────┐
│                      Frontend (React)                        │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────────┐  │
│  │ SearchResult│───▶│ SummaryBtn  │───▶│ SummaryDisplay  │  │
│  └─────────────┘    └─────────────┘    └─────────────────┘  │
└─────────────────────────────────────────────────────────────┘
                              │ IPC
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                   Electron Main Process                      │
│  ┌─────────────────┐    ┌─────────────────────────────────┐ │
│  │ LLMIPCHandlers  │───▶│        LLMService               │ │
│  │                 │    │  - formatMessages()             │ │
│  │ llm:summarize   │    │  - buildPrompt()                │ │
│  │ llm:configure   │    │  - callOllama()                 │ │
│  │ llm:getConfig   │    │  - parseResponse()              │ │
│  │ llm:testConnect │    └─────────────────────────────────┘ │
│  └─────────────────┘                   │                    │
└─────────────────────────────────────────────────────────────┘
                                         │ HTTP
                                         ▼
┌─────────────────────────────────────────────────────────────┐
│                 Ollama (远程服务器:11434)                     │
│                       qwen2.5:7b                             │
└─────────────────────────────────────────────────────────────┘
```

### 2.3 数据流

1. 用户在搜索结果页点击"AI 总结"
2. 前端收集当前搜索结果（消息列表）
3. 通过 IPC 调用 `llm:summarize`
4. LLMService 格式化消息，构建 Prompt
5. 调用 Ollama API (`POST /api/chat`)
6. 解析响应，返回总结内容
7. 前端展示总结结果

### 2.4 Ollama API 接口

```typescript
// 请求
POST http://localhost:11434/api/chat
{
  "model": "qwen2.5:7b",
  "messages": [
    { "role": "system", "content": "你是一个消息总结助手..." },
    { "role": "user", "content": "请总结以下消息：\n..." }
  ],
  "stream": false
}

// 响应
{
  "message": {
    "role": "assistant",
    "content": "## 总结\n\n..."
  }
}
```

---

## 3. 功能规格

### 3.1 配置项

| 配置项 | 默认值 | 说明 |
|--------|--------|------|
| ollamaUrl | `http://<服务器IP>:11434` | Ollama 服务地址（远程服务器） |
| model | `qwen2.5:7b` | 使用的模型名称 |
| maxTokens | 2048 | 最大输出 token 数 |
| temperature | 0.3 | 生成温度（较低以保证稳定性） |
| timeout | 120000 | 请求超时时间（毫秒） |

### 3.2 Prompt 模板

```markdown
你是一个专业的消息总结助手。请根据以下来自不同平台的消息，生成一份结构化的总结。

## 要求
1. 提取关键信息和要点
2. 识别重要的人物、时间、事件
3. 如有行动项或待办事项，请单独列出
4. 保持客观，不添加原消息中没有的信息
5. 使用中文回复

## 消息列表
{messages}

## 请输出总结
```

### 3.3 消息格式化

将搜索结果格式化为 LLM 可理解的文本：

```markdown
---
[平台: Gmail] [发送者: alice@example.com] [时间: 2024-01-15 10:30]
消息内容在这里...
---
[平台: Slack] [发送者: Bob] [时间: 2024-01-15 11:00]
另一条消息内容...
---
```

### 3.4 交互流程

采用**手动触发**方案，用户主动点击按钮触发总结。

```
用户输入关键词 → 点击搜索
        ↓
   并行搜索各平台（Gmail、Slack、Lark）
        ↓
   流式渲染消息（实时显示搜索结果）
        ↓
   搜索完成后，结果区域顶部显示：
   ┌──────────────────────────────────────────┐
   │ 找到 25 条消息           [✨ AI 总结]    │
   └──────────────────────────────────────────┘
        ↓
   用户点击 [AI 总结] 按钮
        ↓
   按钮变为加载状态，显示"正在总结..."
        ↓
   LLM 返回结果，在消息列表上方展示总结卡片
```

**选择手动触发的理由：**
- 搜索结果可能很少，用户不需要总结
- 用户可能频繁搜索调整关键词，自动总结浪费资源
- LLM 响应需要 10-30 秒，自动总结会拖慢体验

### 3.5 UI 设计

#### 搜索结果页 - 总结按钮
- 位置：搜索结果统计栏右侧
- 状态：
  - **默认**：显示"✨ AI 总结"
  - **加载中**：显示 spinner + "正在总结..."
  - **已完成**：按钮变为"重新总结"
  - **错误**：显示错误提示 + "重试"
- 禁用条件：
  - 搜索进行中（未完成）
  - 无搜索结果
  - LLM 服务未配置

#### 总结结果展示
- 位置：消息列表上方，搜索统计栏下方
- 展示方式：可折叠的卡片组件
- 支持 Markdown 渲染
- 显示生成耗时（如"耗时 15.2 秒"）
- 支持复制到剪贴板
- 支持关闭/隐藏

#### 设置页 - LLM 配置
- Ollama 服务地址输入
- 模型选择下拉框
- 连接测试按钮
- 参数调整（高级选项）

### 3.5 错误处理

| 错误场景 | 处理方式 |
|----------|----------|
| Ollama 未运行 | 提示用户启动 Ollama 服务 |
| 模型未安装 | 提示用户执行 `ollama pull` 命令 |
| 请求超时 | 提示超时，建议减少消息数量 |
| 内存不足 | 提示关闭其他程序或使用更小模型 |

---

## 4. 实施计划

### 4.1 开发阶段

| 阶段 | 任务 | 产出 |
|------|------|------|
| 阶段 1 | 后端服务开发 | LLMService、IPC Handlers |
| 阶段 2 | 前端 UI 开发 | 总结按钮、结果展示、设置页 |
| 阶段 3 | 集成测试 | 端到端功能验证 |
| 阶段 4 | 优化完善 | 错误处理、性能优化 |

### 4.2 详细任务清单

#### 阶段 1：后端服务开发

- [x] **1.1 创建类型定义** (`src/types/llm.ts`) ✅
  - LLM 配置接口
  - 总结请求/响应接口
  - 错误类型定义

- [x] **1.2 开发 LLMService** (`electron/services/LLMService.ts`) ✅
  - 实现 Ollama API 调用
  - 消息格式化方法
  - Prompt 构建方法
  - 响应解析方法
  - 错误处理

- [x] **1.3 开发 IPC Handlers** (`electron/ipc/LLMIPCHandlers.ts`) ✅
  - `llm:summarize` - 执行总结
  - `llm:configure` - 保存配置
  - `llm:getConfig` - 获取配置
  - `llm:testConnection` - 测试连接
  - `llm:getModels` - 获取可用模型列表

- [x] **1.4 集成到 ServiceManager** ✅
  - 注册 LLMService
  - 注册 IPC Handlers

- [x] **1.5 扩展配置存储** ✅
  - 在 ConfigurationService 中添加 LLM 配置

#### 阶段 2：前端 UI 开发

- [ ] **2.1 创建 LLM Store** (`src/store/llmStore.ts`)
  - 配置状态管理
  - 总结状态管理
  - 加载/错误状态

- [x] **2.2 开发总结按钮组件** (`src/components/SummaryButton.tsx`) ✅
  - 按钮 UI
  - 加载状态显示
  - 点击事件处理

- [x] **2.3 开发总结结果组件** (`src/components/SummaryResult.tsx`) ✅
  - Markdown 渲染
  - 折叠/展开功能
  - 复制功能
  - 耗时显示

- [x] **2.4 开发 LLM 设置组件** (`src/components/LLMSettings.tsx`) ✅
  - 服务地址配置
  - 模型选择
  - 连接测试
  - 高级参数

- [x] **2.5 集成到搜索结果页** ✅
  - 添加总结按钮到 SearchResults
  - 添加总结结果展示区域

- [x] **2.6 集成到设置页** ✅
  - 添加 LLM 设置 Tab 或 Section

#### 阶段 3：集成测试

- [ ] **3.1 编写单元测试**
  - LLMService 测试
  - 消息格式化测试
  - Prompt 构建测试

- [ ] **3.2 端到端测试**
  - 完整流程测试
  - 错误场景测试

#### 阶段 4：优化完善

- [ ] **4.1 性能优化**
  - 消息截断策略（超长消息处理）
  - ~~响应流式显示（可选）~~ ✅ 已实现

- [ ] **4.2 用户体验优化**
  - 加载动画
  - 进度提示
  - 快捷键支持

- [ ] **4.3 文档更新**
  - 更新 README
  - 更新 CLAUDE.md
  - 用户使用指南

---

## 5. 文件清单

### 5.1 新增文件

- [x] `src/types/llm.ts` - LLM 相关类型定义 ✅
- [x] `electron/services/LLMService.ts` - LLM 服务核心逻辑 ✅
- [x] `electron/ipc/LLMIPCHandlers.ts` - IPC 通信处理 ✅
- [ ] `src/store/llmStore.ts` - 前端状态管理
- [x] `src/components/SummaryButton.tsx` - 总结按钮组件 ✅
- [x] `src/components/SummaryResult.tsx` - 总结结果展示组件 ✅
- [x] `src/components/LLMSettings.tsx` - LLM 设置组件 ✅
- [ ] `electron/services/__tests__/LLMService.test.ts` - 单元测试

### 5.2 修改文件

- [ ] `electron/services/ServiceManager.ts` - 注册 LLMService
- [x] `electron/main.ts` - 注册 IPC Handlers ✅
- [x] `src/types/global.d.ts` - 添加 IPC 类型声明 ✅
- [x] `src/pages/SearchResults.tsx` - 集成总结功能（或对应的结果页面）✅
- [x] `src/pages/Settings.tsx` - 添加 LLM 设置（或对应的设置页面）✅

---

## 6. 前置条件

### 6.1 服务器环境要求

Ollama 已部署在远程服务器，需确保：

- [ ] Ollama 服务正在运行
- [ ] 模型已拉取：`ollama pull qwen2.5:7b`
- [ ] 端口 11434 对客户端可访问（检查防火墙设置）
- [ ] Ollama 配置允许远程访问（设置 `OLLAMA_HOST=0.0.0.0`）

### 6.2 客户端要求

- [ ] 能够访问 Ollama 服务器的网络连接
- [ ] 在应用设置中配置正确的服务器地址

### 6.3 验证清单

- [ ] 检查 Ollama 服务是否可访问
  ```bash
  curl http://<服务器IP>:11434/api/tags
  ```

- [ ] 测试模型是否正常响应
  ```bash
  curl http://<服务器IP>:11434/api/chat -d '{
    "model": "qwen2.5:7b",
    "messages": [{"role": "user", "content": "你好"}],
    "stream": false
  }'
  ```

- [ ] 确认响应时间在可接受范围内（建议 < 60s）

---

## 7. 风险与应对

- [ ] **网络连接不稳定** → 添加重试机制、超时提示
- [ ] **服务器不可达** → 连接测试、清晰的错误提示
- [ ] **消息过多超出上下文** → 实现消息截断/分批策略
- [ ] **Ollama 版本兼容** → 指定最低版本要求、兼容性处理

---

## 8. 后续扩展（可选）

- [ ] 支持流式响应，实时显示生成内容
- [ ] 支持多种总结模板（简洁版/详细版/行动项）
- [ ] 支持云端 API（OpenAI、Claude）作为备选
- [ ] 支持选中部分消息进行总结
- [ ] 总结历史记录保存
- [ ] 支持导出总结为文件

---

## 更新记录

| 日期 | 版本 | 说明 |
|------|------|------|
| 2024-12-23 | v1.0 | 初始版本 |
